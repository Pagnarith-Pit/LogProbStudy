Comparison Models:
We will be generating log probability from this: 

+ Qwen/Qwen3-30B-A3B-Thinking-2507-FP8
+ microsoft/Phi-4-reasoning-plus

Non-thinking 

+ openai/gpt-oss-20b
+ google/gemma-3-12b-it
+ meta-llama/Llama-3.2-3B-Instruct


Our control isn't about large LLM anymore. We now do ablation with null responses. 


Mixture of models: thinking, non-thinking
Mixture of sizes: 3B to 30B